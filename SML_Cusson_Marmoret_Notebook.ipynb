{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Project\n",
    "## CUSSON Thomas - MARMORET Axel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the needed imports\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# The differents algorithms\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewList = []\n",
    "\n",
    "mydir = \"../Datasets/petit/txt_sentoken/\" # Could need to change the path\n",
    "\n",
    "# Load the positive examples \n",
    "for txt in listdir(mydir + \"pos/\"):\n",
    "    if isfile(join(mydir + \"pos/\", txt)):\n",
    "        review = open(join(mydir + \"pos/\", txt), \"r\")\n",
    "        reviews = \" \".join(review.readlines()).replace(\"\\n\", \" \")\n",
    "        reviewList.append((reviews, 0))\n",
    "        \n",
    "# Load the negative examples\n",
    "for txt in listdir(mydir + \"neg/\"):\n",
    "    if isfile(join(mydir + \"neg/\", txt)):\n",
    "        review = open(join(mydir + \"neg/\", txt), \"r\")\n",
    "        reviews = \" \".join(review.readlines()).replace(\"\\n\", \" \")\n",
    "        reviewList.append((reviews, 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the positive and negative examples\n",
    "random.seed(42)\n",
    "random.shuffle(reviewList)\n",
    "\n",
    "# Use first 70% for training\n",
    "train_size =int(0.7 * len(reviewList))\n",
    "train_set, test_set = reviewList[:train_size], reviewList[train_size:]\n",
    "        \n",
    "# Split the datasets in usable tables\n",
    "train_set_unlabeled,train_labels,test_set_unlabeled,test_labels = [], [], [], []\n",
    "\n",
    "for i in range(len(train_set)) :\n",
    "    train_set_unlabeled.append(train_set[i][0])\n",
    "    train_labels.append(train_set[i][1])\n",
    "for i in range(len(test_set)) :\n",
    "    test_set_unlabeled.append(test_set[i][0])\n",
    "    test_labels.append(test_set[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline of action specific for the Naive Bayes algorithm\n",
    "pipeline_nb = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=0.01, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "# NB : We have run our algorithm with min_df as a parameter in order to find its best value\n",
    "# It gaves us 16 in range (1,51,5) and 15 in range (13, 19)\n",
    "# This parameter exploded our computation time,\n",
    "# so we decided to fixed it at 1% of our training dataset size (14 here) in order to run our algorithms in decent time\n",
    "\n",
    "# GridSearch allows us to test severals parameters\n",
    "parameters_nb = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'nb__alpha': (1e-1, 1e-2),\n",
    "}\n",
    "\n",
    "GridSearchNaiveBayse = GridSearchCV(pipeline_nb, parameters_nb, n_jobs=-1, cv=KFold(n_splits=5, random_state=0), return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test pipeline !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchNaiveBayse = GridSearchNaiveBayse.fit(train_set_unlabeled, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from the different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_naive = GridSearchNaiveBayse.cv_results_\n",
    "\n",
    "pd.DataFrame(results_naive).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_results = pd.DataFrame({\"rank\":results_naive['rank_test_score'],\n",
    "                                  \"alpha\":results_naive[\"param_nb__alpha\"],\n",
    "                                  \"tfidf\":results_naive[\"param_tfidf__use_idf\"],\n",
    "                                  \"n_gram\":results_naive[\"param_vect__ngram_range\"],\n",
    "                                  \"mean_test_score\":results_naive[\"mean_test_score\"],\n",
    "                                  \"mean_train_score\":results_naive[\"mean_train_score\"]}).set_index(\"rank\")\n",
    "\n",
    "comparison_results.sort_values('rank', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters_nb.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchNaiveBayse.best_params_[param_name]))\n",
    "\n",
    "print(\"Best Estimator : \" + str(GridSearchNaiveBayse.best_estimator_.get_params()[\"nb\"]))\n",
    "\n",
    "print(\"Best Score on those conditions : \" + str(GridSearchNaiveBayse.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping the best estimator according to the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = GridSearchNaiveBayse.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the prediction on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bayse_prediction = naive.predict(test_set_unlabeled)\n",
    "\n",
    "print(\"Accuracy : \" + str(metrics.accuracy_score(test_bayse_prediction, test_labels)))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(test_labels, test_bayse_prediction, normalize=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Recall : \" + str(metrics.recall_score(test_bayse_prediction, test_labels)))\n",
    "print(\"Precision : \" + str(metrics.precision_score(test_bayse_prediction, test_labels)))\n",
    "print(\"Under ROC curve area : \" + str(metrics.roc_auc_score(test_bayse_prediction, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know use the same operating procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting\n",
    "pipeline_svm = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=0.01, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('svc', SVC()),\n",
    "                    ])\n",
    "\n",
    "parameters_svm = {'tfidf__use_idf': (True, False),\n",
    "                  'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'svc__kernel': ['linear','rbf'],\n",
    "                  'svc__gamma': [0.1, 0.01],\n",
    "                  'svc__C': [10, 100],\n",
    "}\n",
    "\n",
    "GridSearchSVM = GridSearchCV(pipeline_svm, parameters_svm, n_jobs=-1, cv=KFold(n_splits=5, random_state=0))\n",
    "\n",
    "GridSearchSVM.fit(train_set_unlabeled, train_labels)\n",
    "\n",
    "results_SVM = GridSearchSVM.cv_results_\n",
    "\n",
    "comparison_results = pd.DataFrame({\"rank\":results_SVM['rank_test_score'],\n",
    "                                  \"kernel\":results_SVM[\"param_svc__kernel\"],\n",
    "                                  \"gamma\":results_SVM[\"param_svc__gamma\"],\n",
    "                                  \"C\":results_SVM[\"param_svc__C\"],\n",
    "                                  \"tfidf\":results_SVM[\"param_tfidf__use_idf\"],\n",
    "                                  \"n_gram\":results_SVM[\"param_vect__ngram_range\"],\n",
    "                                  \"mean_test_score\":results_SVM[\"mean_test_score\"],\n",
    "                                  \"mean_train_score\":results_SVM[\"mean_train_score\"]}).set_index(\"rank\")\n",
    "\n",
    "comparison_results.sort_values('rank', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "for param_name in sorted(parameters_svm.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchSVM.best_params_[param_name]))\n",
    "\n",
    "print(\"Best Estimator : \" + str(GridSearchSVM.best_estimator_.get_params()[\"svc\"]))\n",
    "\n",
    "svm = GridSearchSVM.best_estimator_\n",
    "\n",
    "# Prediction on test set\n",
    "test_svm_prediction = svm.predict(test_set_unlabeled)\n",
    "\n",
    "# Results\n",
    "print(\"\\nAccuracy : \" + str(metrics.accuracy_score(test_svm_prediction, test_labels)))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(test_labels, test_svm_prediction, normalize=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Recall : \" + str(metrics.recall_score(test_svm_prediction, test_labels)))\n",
    "print(\"Precision : \" + str(metrics.precision_score(test_svm_prediction, test_labels)))\n",
    "\n",
    "print(\"Under ROC curve area : \" + str(metrics.roc_auc_score(test_svm_prediction, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting\n",
    "pipeline_rf = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=0.01, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('rf', RandomForestClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters_rf = {'tfidf__use_idf': (True, False),\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'rf__n_estimators': [1000, 2000, 3000],\n",
    "}\n",
    "\n",
    "GridSearchRF = GridSearchCV(pipeline_rf, parameters_rf, n_jobs=-1, cv=KFold(n_splits=5, random_state=0))\n",
    "\n",
    "GridSearchRF.fit(train_set_unlabeled, train_labels)\n",
    "\n",
    "results_RF = GridSearchRF.cv_results_\n",
    "\n",
    "comparison_results = pd.DataFrame({\"rank\":results_RF['rank_test_score'],\n",
    "                                  \"n_estimators\":results_RF[\"param_rf__n_estimators\"],\n",
    "                                  \"tfidf\":results_RF[\"param_tfidf__use_idf\"],\n",
    "                                  \"n_gram\":results_RF[\"param_vect__ngram_range\"],\n",
    "                                  \"mean_test_score\":results_RF[\"mean_test_score\"],\n",
    "                                  \"mean_train_score\":results_RF[\"mean_train_score\"]}).set_index(\"rank\")\n",
    "\n",
    "comparison_results.sort_values('rank', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "for param_name in sorted(parameters_rf.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchRF.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchRF.best_estimator_.get_params()[\"rf\"]))\n",
    "\n",
    "rf = GridSearchRF.best_estimator_\n",
    "\n",
    "# Prediction on test set\n",
    "test_rf_prediction = rf.predict(test_set_unlabeled)\n",
    "\n",
    "# Results\n",
    "print(\"\\nAccuracy : \" + str(metrics.accuracy_score(test_rf_prediction, test_labels)))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(test_labels, test_rf_prediction, normalize=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Recall : \" + str(metrics.recall_score(test_rf_prediction, test_labels)))\n",
    "print(\"Precision : \" + str(metrics.precision_score(test_rf_prediction, test_labels)))\n",
    "\n",
    "print(\"Under ROC curve area : \" + str(metrics.roc_auc_score(test_rf_prediction, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting\n",
    "pipeline_knn = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=0.01, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('knn', KNeighborsClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters_knn = {'tfidf__use_idf': (True, False),\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'knn__p': [1,2],\n",
    "                 'knn__n_neighbors': [25,50,100],\n",
    "}\n",
    "\n",
    "GridSearchKNN = GridSearchCV(pipeline_knn, parameters_knn, n_jobs=-1, cv=KFold(n_splits=5, random_state=0))\n",
    "\n",
    "GridSearchKNN.fit(train_set_unlabeled, train_labels)\n",
    "\n",
    "results_KNN = GridSearchKNN.cv_results_\n",
    "\n",
    "comparison_results = pd.DataFrame({\"rank\":results_KNN['rank_test_score'],\n",
    "                                  \"p\":results_KNN[\"param_knn__p\"],\n",
    "                                  \"n_neighbors\":results_KNN[\"param_knn__n_neighbors\"],\n",
    "                                  \"tfidf\":results_KNN[\"param_tfidf__use_idf\"],\n",
    "                                  \"n_gram\":results_KNN[\"param_vect__ngram_range\"],\n",
    "                                  \"mean_test_score\":results_KNN[\"mean_test_score\"],\n",
    "                                  \"mean_train_score\":results_KNN[\"mean_train_score\"]}).set_index(\"rank\")\n",
    "\n",
    "comparison_results.sort_values('rank', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "for param_name in sorted(parameters_knn.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchKNN.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchKNN.best_estimator_.get_params()[\"knn\"]))\n",
    "\n",
    "knn = GridSearchKNN.best_estimator_\n",
    "\n",
    "# Prediction on test set\n",
    "test_knn_prediction = knn.predict(test_set_unlabeled)\n",
    "\n",
    "# Results\n",
    "print(\"Accuracy : \" + str(metrics.accuracy_score(test_knn_prediction, test_labels)))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(test_labels, test_knn_prediction, normalize=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Recall : \" + str(metrics.recall_score(test_knn_prediction, test_labels)))\n",
    "print(\"Precision : \" + str(metrics.precision_score(test_knn_prediction, test_labels)))\n",
    "\n",
    "print(\"Under ROC curve area : \" + str(metrics.roc_auc_score(test_knn_prediction, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [metrics.accuracy_score(test_bayse_prediction, test_labels),\n",
    "     metrics.accuracy_score(test_svm_prediction, test_labels),\n",
    "     metrics.accuracy_score(test_rf_prediction, test_labels),\n",
    "     metrics.accuracy_score(test_knn_prediction, test_labels)\n",
    "]\n",
    "\n",
    "labels = ['Naive Bayse', 'SVM', 'Random Forest', 'K Nearest Neighbors']\n",
    "\n",
    "bar_width = 1.0 \n",
    "\n",
    "plt.xticks(x, labels, rotation=45)\n",
    "\n",
    "plt.margins(0.2)\n",
    "plt.bar(x, y, bar_width, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = naive.get_params()[\"vect\"].get_feature_names()\n",
    "\n",
    "# number of times each word appears across all positive messages\n",
    "pos_word_count = naive.get_params()[\"nb\"].feature_count_[0, :]\n",
    "\n",
    "neg_word_count = naive.get_params()[\"nb\"].feature_count_[1, :]\n",
    "\n",
    "ratio_pos_neg = []\n",
    "for i in range(len(pos_word_count)) :\n",
    "    ratio_pos_neg.append(pos_word_count[i]/neg_word_count[i])\n",
    "\n",
    "words_frequencies = pd.DataFrame({\"word\":words, \"positive\":pos_word_count, \"negative\":neg_word_count, \"ratio\":ratio_pos_neg}).set_index(\"word\")\n",
    "\n",
    "words_frequencies.sort_values('ratio', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
