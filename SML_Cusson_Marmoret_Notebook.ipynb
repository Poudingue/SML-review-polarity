{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Project\n",
    "## CUSSON Thomas - MARMORET Axel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the needed imports\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "\n",
    "# The differents algorithms\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Some important variables\n",
    "listResults = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewList = []\n",
    "\n",
    "mydir = \"../Datasets/petit/txt_sentoken/\" # Could need to change the path\n",
    "\n",
    "# Load the positive examples \n",
    "for txt in listdir(mydir + \"pos/\"):\n",
    "    if isfile(join(mydir + \"pos/\", txt)):\n",
    "        review = open(join(mydir + \"pos/\", txt), \"r\")\n",
    "        reviews = \" \".join(review.readlines()).replace(\"\\n\", \" \")\n",
    "        reviewList.append((reviews, 0))\n",
    "        \n",
    "# Load the negative examples\n",
    "for txt in listdir(mydir + \"neg/\"):\n",
    "    if isfile(join(mydir + \"neg/\", txt)):\n",
    "        review = open(join(mydir + \"neg/\", txt), \"r\")\n",
    "        reviews = \" \".join(review.readlines()).replace(\"\\n\", \" \")\n",
    "        reviewList.append((reviews, 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the positive and negative examples\n",
    "random.shuffle(reviewList)\n",
    "\n",
    "# Use first 70% for training\n",
    "train_size =int(0.7 * len(reviewList))\n",
    "train_set, test_set = reviewList[:train_size], reviewList[train_size:]\n",
    "        \n",
    "# Separate the datasets in usable tables\n",
    "train_set_unlabeled,train_labels,test_set_unlabeled,test_labels = [], [], [], []\n",
    "\n",
    "for i in range(len(train_set)) :\n",
    "    train_set_unlabeled.append(train_set[i][0])\n",
    "    train_labels.append(train_set[i][1])\n",
    "for i in range(len(test_set)) :\n",
    "    test_set_unlabeled.append(test_set[i][0])\n",
    "    test_labels.append(test_set[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a pipeline with some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline of action specific for the Naive Bayes algorithm\n",
    "pipeline_nb = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=15, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "# NB : We have run our algorithm with min_df as a parameter in order to find its best value\n",
    "# It gaves us 16 in range (1,51,5) and 15 in range (13, 19)\n",
    "# This parameter exploded our computation time, so we decided to fixed it at 15 in order to run our algorithms in decent time\n",
    "\n",
    "# GridSearch allows us to test severals parameters\n",
    "parameters_nb = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'nb__alpha': (1e-1, 1e-2),\n",
    "}\n",
    "\n",
    "GridSearchNaiveBayse = GridSearchCV(pipeline_nb, parameters_nb, n_jobs=-1, cv = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchNaiveBayse = GridSearchNaiveBayse.fit(train_set_unlabeled, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the results : searching for the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters_nb.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchNaiveBayse.best_params_[param_name]))\n",
    "\n",
    "print(\"Best Estimator : \" + str(GridSearchNaiveBayse.best_estimator_.get_params()[\"nb\"]))\n",
    "\n",
    "print (\"Best Score on those conditions : \" + str(GridSearchNaiveBayse.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = GridSearchNaiveBayse.best_estimator_\n",
    "\n",
    "test_bayse_prediction = naive.predict(test_set_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy : \" + str(metrics.accuracy_score(test_bayse_prediction, test_labels)))\n",
    "print(\"Recall : \" + str(metrics.recall_score(test_bayse_prediction, test_labels)))\n",
    "print(\"Precision : \" + str(metrics.precision_score(test_bayse_prediction, test_labels)))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(test_labels, test_bayse_prediction, normalize=True)\n",
    "plt.show()\n",
    "#Si ça plante, installer matplotlib (https://scikit-plot.readthedocs.io/en/stable/Quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC_Curve : \" + str(metrics.roc_auc_score(test_bayse_prediction, test_labels)))\n",
    "\n",
    "#plt.hist([((metrics.roc_auc_score(test_bayse_prediction, test_labels))), ((metrics.recall_score(test_bayse_prediction, test_labels)))])\n",
    "#plt.ylabel('some numbers')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = naive.get_params()[\"vect\"].get_feature_names()\n",
    "\n",
    "# number of times each word appears across all positive messages\n",
    "pos_word_count = naive.get_params()[\"nb\"].feature_count_[0, :]\n",
    "\n",
    "neg_word_count = naive.get_params()[\"nb\"].feature_count_[1, :]\n",
    "\n",
    "words_frequencies = pd.DataFrame({\"word\":words, \"positive\":pos_word_count, \"negative\":neg_word_count}).set_index(\"word\")\n",
    "words_frequencies.head()\n",
    "\n",
    "words_frequencies.sort_values('positive', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=25, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('svc', SVC()),\n",
    "                    ])\n",
    "\n",
    "parameters_svm = {'tfidf__use_idf': (True, False),\n",
    "                  'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'svc__kernel': ['linear','rbf'],\n",
    "                  'svc__gamma': [0.1, 0.01],\n",
    "                  'svc__C': [1, 10, 100],\n",
    "}\n",
    "\n",
    "GridSearchSVM = GridSearchCV(pipeline_svm, parameters_svm, n_jobs=-1)\n",
    "\n",
    "GridSearchSVM.fit(train_set_unlabeled, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters for the SVM\n",
    "for param_name in sorted(parameters_svm.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchSVM.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchSVM.best_estimator_.get_params()[\"svc\"]))\n",
    "\n",
    "svm = GridSearchSVM.best_estimator_\n",
    "\n",
    "test_svm_prediction = svm.predict(test_set_unlabeled)\n",
    "\n",
    "print(\"Accuracy : \" + str(metrics.accuracy_score(test_svm_prediction, test_labels)))\n",
    "print(\"Recall : \" + str(metrics.recall_score(test_svm_prediction, test_labels)))\n",
    "print(\"Precision : \" + str(metrics.precision_score(test_svm_prediction, test_labels)))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(test_labels, test_svm_prediction, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=25, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('rf', RandomForestClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters_rf = {'tfidf__use_idf': (True, False),\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'rf__n_estimators': [100, 1000, 2000],\n",
    "}\n",
    "\n",
    "GridSearchRF = GridSearchCV(pipeline_rf, parameters_rf, n_jobs=-1)\n",
    "\n",
    "GridSearchRF.fit(train_set_unlabeled, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters_rf.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchRF.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchRF.best_estimator_.get_params()[\"rf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = GridSearchRF.best_estimator_\n",
    "\n",
    "test_rf_prediction = rf.predict(test_set_unlabeled)\n",
    "\n",
    "print(\"Accuracy : \" + str(metrics.accuracy_score(test_rf_prediction, test_labels)))\n",
    "print(\"Recall : \" + str(metrics.recall_score(test_rf_prediction, test_labels)))\n",
    "print(\"Precision : \" + str(metrics.precision_score(test_rf_prediction, test_labels)))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(test_labels, test_rf_prediction, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gb = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=25, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('gb', GradientBoostingClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters_gb = {'tfidf__use_idf': (True, False),\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'gb__learning_rate': [1e-1, 1e-2, 1e-3],\n",
    "                 'gb__n_estimators': [100, 1000, 10000],\n",
    "}\n",
    "\n",
    "GridSearchGB = GridSearchCV(pipeline_gb, parameters_gb, n_jobs=-1, scoring=)\n",
    "\n",
    "GridSearchGB.fit(train_set_unlabeled, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters_rf.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchRF.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchRF.best_estimator_.get_params()[\"gb\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters for the SVM\n",
    "gb = GridSearchGB.best_estimator_\n",
    "\n",
    "test_gb_prediction = gb.predict(test_set_unlabeled)\n",
    "\n",
    "print(\"Accuracy : \" + str(metrics.accuracy_score(test_gb_prediction, test_labels)))\n",
    "print(\"Recall : \" + str(metrics.recall_score(test_gb_prediction, test_labels)))\n",
    "print(\"Precision : \" + str(metrics.precision_score(test_gb_prediction, test_labels)))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(test_labels, test_gb_prediction, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResults.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rajouter Precision et Rappel a minima\n",
    "\n",
    "# Matrice de confusion\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# ROC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Voir section \"Examining a model for further insight\" du prof, tokenisation par ex\n",
    "\n",
    "# Dessiner les résultats (histogramme avec tous les scores par exemple)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
