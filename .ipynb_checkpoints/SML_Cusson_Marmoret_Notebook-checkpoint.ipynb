{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Project\n",
    "## CUSSON Thomas - MARMORET Axel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the needed imports\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The differents algorithms\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Some important variables\n",
    "listResults = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewList = []\n",
    "\n",
    "mydir = \"../Datasets/petit/txt_sentoken/\" # Could need to change the path\n",
    "\n",
    "# Load the positive examples \n",
    "for txt in listdir(mydir + \"pos/\"):\n",
    "    if isfile(join(mydir + \"pos/\", txt)):\n",
    "        review = open(join(mydir + \"pos/\", txt), \"r\")\n",
    "        reviews = \" \".join(review.readlines()).replace(\"\\n\", \" \")\n",
    "        reviewList.append((reviews, 0))\n",
    "        \n",
    "# Load the negative examples\n",
    "for txt in listdir(mydir + \"neg/\"):\n",
    "    if isfile(join(mydir + \"neg/\", txt)):\n",
    "        review = open(join(mydir + \"neg/\", txt), \"r\")\n",
    "        reviews = \" \".join(review.readlines()).replace(\"\\n\", \" \")\n",
    "        reviewList.append((reviews, 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the positive and negative examples\n",
    "random.shuffle(reviewList)\n",
    "\n",
    "# Use first 70% for training\n",
    "train_size =int(0.7 * len(reviewList))\n",
    "train_set, test_set = reviewList[:train_size], reviewList[train_size:]\n",
    "        \n",
    "vect_sw = CountVectorizer(stop_words='english', min_df=25, analyzer = \"word\", ngram_range = (1, 1)) # Jouer avec les valeurs de min et max df\n",
    "\n",
    "# Separate the datasets in usable tables\n",
    "train_set_unlabeled,train_labels,test_set_unlabeled,test_labels = [], [], [], []\n",
    "\n",
    "for i in range(len(train_set)) :\n",
    "    train_set_unlabeled.append(train_set[i][0])\n",
    "    train_labels.append(train_set[i][1])\n",
    "for i in range(len(test_set)) :\n",
    "    test_set_unlabeled.append(test_set[i][0])\n",
    "    test_labels.append(test_set[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier (as a comparison tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy='stratified', random_state=None)\n",
    "\n",
    "dummy = dummy.fit(train_set_unlabeled, train_labels)\n",
    "\n",
    "pred = dummy.predict(test_set_unlabeled)\n",
    "\n",
    "print(\"Score : \" + str(dummy.score(pred, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a pipeline with some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline of action specific for the Naive Bayes algorithm\n",
    "pipeline_nb = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer = \"word\")), #min_df=25,\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('nb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "# GridSearch allows us to test severals parameters\n",
    "parameters_nb = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 #'vect__min_df':range(14,19), #range(1,51,5) donne 16\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'nb__alpha': (1e-1, 1e-2),\n",
    "}\n",
    "\n",
    "GridSearchNaiveBayse = GridSearchCV(pipeline_nb, parameters_nb, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchNaiveBayse = GridSearchNaiveBayse.fit(train_set_unlabeled, train_labels)\n",
    "\n",
    "GridSearchNaiveBayse.predict(test_set_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Best Score : \" + str(GridSearchNaiveBayse.best_score_))\n",
    "\n",
    "listResults.append(GridSearchNaiveBayse.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the results : searching for the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters_nb.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchNaiveBayse.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchNaiveBayse.best_estimator_.get_params()[\"nb\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=25, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('svc', SVC()),\n",
    "                    ])\n",
    "\n",
    "parameters_svm = {'tfidf__use_idf': (True, False),\n",
    "                  'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'svc__kernel': ['linear','rbf'],\n",
    "                  'svc__gamma': [0.1, 0.01],\n",
    "                  'svc__C': [1, 10, 100],\n",
    "}\n",
    "\n",
    "GridSearchSVM = GridSearchCV(pipeline_svm, parameters_svm, n_jobs=-1)\n",
    "\n",
    "GridSearchSVM.fit(train_set_unlabeled, train_labels)\n",
    "\n",
    "GridSearchSVM.predict(test_set_unlabeled)\n",
    "\n",
    "print(\"Best Score : \" + str(GridSearchSVM.best_score_))\n",
    "\n",
    "listResults.append(GridSearchSVM.best_score_)\n",
    "\n",
    "# Get the best parameters for the SVM\n",
    "for param_name in sorted(parameters_svm.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchSVM.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchSVM.best_estimator_.get_params()[\"svc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=25, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('rf', RandomForestClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters_rf = {'tfidf__use_idf': (True, False),\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'rf__n_estimators': [100, 1000, 2000],\n",
    "}\n",
    "\n",
    "GridSearchRF = GridSearchCV(pipeline_rf, parameters_rf, n_jobs=-1)\n",
    "\n",
    "GridSearchRF.fit(train_set_unlabeled, train_labels)\n",
    "\n",
    "GridSearchRF.predict(test_set_unlabeled)\n",
    "\n",
    "print(\"Best Score : \" + str(GridSearchRF.best_score_))\n",
    "\n",
    "listResults.append(GridSearchRF.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters_rf.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchRF.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchRF.best_estimator_.get_params()[\"rf\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gb = Pipeline([('vect', CountVectorizer(stop_words='english', min_df=25, analyzer = \"word\")),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('gb', GradientBoostingClassifier()),\n",
    "                    ])\n",
    "\n",
    "parameters_gb = {'tfidf__use_idf': (True, False),\n",
    "                 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'gb__learning_rate': [1e-1, 1e-2, 1e-3],\n",
    "                 'gb__n_estimators': [100, 1000, 10000],\n",
    "}\n",
    "\n",
    "GridSearchGB = GridSearchCV(pipeline_gb, parameters_gb, n_jobs=-1, scoring=)\n",
    "\n",
    "GridSearchGB.fit(train_set_unlabeled, train_labels)\n",
    "\n",
    "GridSearchGB.predict(test_set_unlabeled)\n",
    "\n",
    "print(\"Best Score : \" + str(GridSearchRF.best_score_))\n",
    "\n",
    "for param_name in sorted(parameters_rf.keys()):\n",
    "    print(\"%s: %r\" % (param_name, GridSearchRF.best_params_[param_name]))\n",
    "\n",
    "print(\" Best Estimator : \" + str(GridSearchRF.best_estimator_.get_params()[\"gb\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResults.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rajouter Precision et Rappel a minima\n",
    "\n",
    "# Matrice de confusion\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# ROC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Voir section \"Examining a model for further insight\" du prof, tokenisation par ex\n",
    "\n",
    "# Dessiner les résultats (histogramme avec tous les scores par exemple)\n",
    "\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_confusion_matrix(y, predictions, normalize=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(listResults)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(recall_score(test_labels, prediction, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
